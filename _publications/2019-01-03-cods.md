---
title: "Leveraging Ontological Knowledge for Neural Language Models"
collection: publications
permalink: /publications/leveragingontologies
excerpt: 'We propose and experiment with methods to combine the use of ontologies and function approximaters. We pose the methods as a type of data-knowledge trade-off and achieve superior performance on muliple tasks'
date: 2019-01-03
venue: "Young Researchers' Symposium, CoDS-COMAD 2019, Kolkata, India"
paperurl: 'https://dl.acm.org/citation.cfm?id=3297059'
# citation: 'Deshpande, A., & Jegadeesan, M. (2019, January). Leveraging Ontological Knowledge for Neural Language Models. In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data (pp. 350-353). ACM.'
---

## Short Summary
- Achieved higher performance (5%) and faster convergence (35%) in Word2Vec by using WordNet for weight-initialization.
- Portrayed enhanced semantic similarity by performing domain-transfer of vectors using [RCM](http://www.aclweb.org/anthology/P14-2089) model and [WordNet Domain](http://wndomains.fbk.eu/download.html).
- Proposed the HRCM model and H-Ordinal Constraints for hierarchy aware vectors to settle the data-knowledge trade-off

## Abstract

Neural Language Models such as Word2Vec and GloVe have been shown to encode semantic relatedness between words. Improvements in unearthing these embeddings can ameliorate performance in numerous downstream applications such as sentiment analysis, question answering, and dialogue generation. Lexical ontologies such as WordNet are known to supply information about semantic similarity rather than relatedness. Further, extracting word embeddings from small corpora is daunting for data-hungry neural networks. This work shows how methods that conflate Word2Vec and Ontologies can achieve better performance, reduce training time and help adapt to domains with a minimum amount of data.

<hr />
